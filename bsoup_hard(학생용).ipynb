{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woghd8503/kernel-academy-web-crawling/blob/main/bsoup_hard(%ED%95%99%EC%83%9D%EC%9A%A9).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrI5woHACWT4"
      },
      "source": [
        "# 웹 데이터 수집 고급 심화: \"Books to Scrape\" 사이트 정복하기 📚🐛 (고급)\n",
        "\n",
        "안녕하세요! 이전 과정들을 통해 웹 스크래핑의 기초와 중급 기술들을 익혔습니다. 이번 고급 심화 과정에서는 스크래핑 연습용으로 만들어진 \"Books to Scrape\" ([http://books.toscrape.com/](http://books.toscrape.com/)) 사이트를 대상으로, 보다 복잡하고 실제적인 스크래핑 작업을 수행해 보겠습니다.\n",
        "\n",
        "**학습 목표**\n",
        "1. 대상 웹사이트의 구조를 분석하고 스크래핑 전략 수립하기.\n",
        "2. 여러 계층(카테고리 목록 -> 책 목록 -> 책 상세)에 걸쳐 데이터 수집하기.\n",
        "3. 페이지네이션(Pagination) 처리, 데이터 클리닝, 상세 오류 처리 등 고급 기법 적용하기.\n",
        "4. 스크래핑 작업을 위한 함수를 모듈화하여 코드의 재사용성 및 가독성 높이기.\n",
        "5. 수집된 데이터를 구조화하고 파일로 저장하기.\n",
        "\n",
        "**준수 사항:** 웹 스크래핑 시에는 항상 해당 웹사이트의 `robots.txt` 파일과 이용 약관을 확인하고, 서버에 과도한 부담을 주지 않도록 주의해야 합니다. \"Books to Scrape\"는 학습 목적으로 자유로운 스크래핑이 허용된 사이트입니다.\n",
        "\n",
        "---"
      ],
      "id": "JrI5woHACWT4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gEwT4-HCWT8"
      },
      "source": [
        "## 📄 1단계: 사이트 분석 및 환경 설정\n",
        "\n",
        "스크래핑을 시작하기 전에 대상 웹사이트인 \"Books to Scrape\"의 구조를 파악하고, 필요한 라이브러리를 준비합니다.\n",
        "\n",
        "**1.1. `robots.txt` 확인**\n",
        "웹사이트의 루트 경로에 `/robots.txt`를 붙여 접속하면 (예: `http://books.toscrape.com/robots.txt`) 해당 파일의 내용을 확인할 수 있습니다. \"Books to Scrape\"의 `robots.txt`는 모든 유저 에이전트(`User-agent: *`)에 대해 모든 경로의 접근을 허용(`Disallow:` 항목 없음)하고 있습니다. 이는 학습용 사이트이므로 매우 관대한 정책입니다.\n",
        "\n",
        "**1.2. 사이트 구조 파악**\n",
        "* **메인 페이지**: 카테고리 목록(사이드바), 책 목록 표시.\n",
        "* **카테고리 페이지**: 특정 카테고리에 속한 책 목록 표시, 페이지네이션 존재.\n",
        "* **책 상세 페이지**: 개별 책의 상세 정보 (제목, 가격, 재고, 설명, UPC, 별점 등) 표시.\n",
        "\n",
        "**1.3. 필요한 라이브러리 임포트 및 기본 설정**"
      ],
      "id": "2gEwT4-HCWT8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRpx838WCWT9_student_imports"
      },
      "outputs": [],
      "source": [
        "# !pip install requests beautifulsoup4 pandas # Colab에 이미 설치되어 있다면 실행 불필요\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin # 상대 URL을 절대 URL로 변환\n",
        "import time # 요청 간 지연시간 설정\n",
        "import re # 정규 표현식 사용\n",
        "import random # User-Agent 랜덤 선택용 (선택 사항)"
      ],
      "id": "jRpx838WCWT9_student_imports"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRpx838WCWT9_student_baseurl"
      },
      "outputs": [],
      "source": [
        "# 기본 URL 설정\n",
        "BASE_URL = ____"
      ],
      "id": "jRpx838WCWT9_student_baseurl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRpx838WCWT9_student_catalogueurl"
      },
      "outputs": [],
      "source": [
        "CATALOGUE_URL = urljoin(BASE_URL, ____) # 책 상세페이지 URL 조립 시 사용"
      ],
      "id": "jRpx838WCWT9_student_catalogueurl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRpx838WCWT9_student_useragent"
      },
      "outputs": [],
      "source": [
        "# User-Agent 설정 (다양한 User-Agent를 준비하여 랜덤으로 사용하는 것도 좋은 방법)\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "]\n",
        "headers = {'User-Agent': random.choice(USER_AGENTS)}"
      ],
      "id": "jRpx838WCWT9_student_useragent"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRpx838WCWT9_student_print_setup"
      },
      "outputs": [],
      "source": [
        "print(f\"스크래핑 기본 URL: {BASE_URL}\")\n",
        "print(f\"사용될 User-Agent: {headers['User-Agent']}\")"
      ],
      "id": "jRpx838WCWT9_student_print_setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QxktJWeCWT_"
      },
      "source": [
        "**1.4. 웹 페이지 요청 함수 정의**\n",
        "\n",
        "안정적인 스크래핑을 위해 `User-Agent` 설정, 요청 간 지연, 기본적인 오류 처리를 포함하는 웹 페이지 요청 함수를 정의합니다."
      ],
      "id": "1QxktJWeCWT_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbTUDPSCWUA_student_func_def"
      },
      "outputs": [],
      "source": [
        "def get_soup(url, retries=3, delay=1):\n",
        "    \"\"\"지정된 URL로부터 BeautifulSoup 객체를 반환하는 함수\"\"\"\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            print(f\"페이지 요청 중 ({i+1}번째 시도): {url}\")\n",
        "            time.sleep(random.uniform(delay, delay + 1))\n",
        "            response = ____(url, headers=headers, timeout=____)\n",
        "            response.____()\n",
        "            return BeautifulSoup(response.content, ____)\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"타임아웃 발생: {url}\")\n",
        "        except ____ as e: # HTTPError 예외 타입\n",
        "            print(f\"HTTP 오류 발생 ({url}): {e.status_code} {e.response.reason}\")\n",
        "            if e.response.status_code == ____: # 404\n",
        "                print(\"404 Not Found. 해당 URL은 존재하지 않습니다.\")\n",
        "                return None # 404 에러 시 재시도 불필요\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"요청 중 오류 발생 ({url}): {e}\")\n",
        "\n",
        "        if i < retries - 1: # 마지막 시도가 아니면 대기 후 재시도\n",
        "            print(f\"{delay * (i+1)}초 후 재시도...\")\n",
        "            ____(delay * (i+1)) # time.sleep\n",
        "        else:\n",
        "            print(f\"최대 재시도 횟수({retries}) 초과. {url} 요청 실패.\")\n",
        "    return None"
      ],
      "id": "VEbTUDPSCWUA_student_func_def"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbTUDPSCWUA_student_func_test_call"
      },
      "outputs": [],
      "source": [
        "# 함수 테스트 (메인 페이지)\n",
        "main_soup = ____(BASE_URL) # get_soup"
      ],
      "id": "VEbTUDPSCWUA_student_func_test_call"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbTUDPSCWUA_student_func_test_print"
      },
      "outputs": [],
      "source": [
        "if main_soup:\n",
        "    print(f\"\\n메인 페이지 제목: {main_soup.title.string.strip()}\")\n",
        "else:\n",
        "    print(\"\\n메인 페이지 로드 실패.\")"
      ],
      "id": "VEbTUDPSCWUA_student_func_test_print"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eQBpk7lCWUA"
      },
      "source": [
        "## 📚 2단계: 카테고리 및 책 URL 수집\n",
        "\n",
        "이제 사이트의 모든 카테고리 링크를 수집하고, 각 카테고리 페이지를 탐색하여 개별 책들의 상세 페이지 URL을 수집합니다."
      ],
      "id": "3eQBpk7lCWUA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BU7baipCWUA"
      },
      "source": [
        "**2.1. 모든 카테고리 URL 수집**\n",
        "\n",
        "메인 페이지의 사이드바에는 책 카테고리 목록이 있습니다. 각 카테고리명과 해당 카테고리 페이지로 연결되는 URL을 추출합니다."
      ],
      "id": "0BU7baipCWUA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UzIZnDmCWUA_student_func_def"
      },
      "outputs": [],
      "source": [
        "def get_category_urls(soup):\n",
        "    \"\"\"메인 페이지 BeautifulSoup 객체에서 모든 카테고리 이름과 URL을 추출하는 함수\"\"\"\n",
        "    category_links = []\n",
        "    if not soup:\n",
        "        return category_links\n",
        "\n",
        "    # 사이드바의 카테고리 목록은 ul.nav-list > li > ul > li > a 형태\n",
        "    # 좀 더 구체적으로는 div.side_categories > ul.nav-list > li > ul\n",
        "    side_categories_ul = soup.select_one(____) # CSS 선택자\n",
        "    if side_categories_ul:\n",
        "        for li_tag in side_categories_ul.find_all('li'):\n",
        "            a_tag = li_tag.find('a')\n",
        "            if a_tag and a_tag.has_attr('href'):\n",
        "                category_name = a_tag.____(strip=True)\n",
        "                category_relative_url = a_tag[____]\n",
        "                category_full_url = ____(BASE_URL, category_relative_url)\n",
        "                category_links.append({'name': category_name, 'url': ____})\n",
        "    return category_links"
      ],
      "id": "-UzIZnDmCWUA_student_func_def"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UzIZnDmCWUA_student_func_test"
      },
      "outputs": [],
      "source": [
        "if main_soup:\n",
        "    all_categories = ____(main_soup) # get_category_urls\n",
        "    print(f\"\\n총 {len(all_categories)}개의 카테고리를 찾았습니다.\")\n",
        "    for cat in all_categories[:3]: # 처음 3개 카테고리 정보 출력\n",
        "        print(cat)\n",
        "else:\n",
        "    all_categories = []\n",
        "    print(\"\\n메인 페이지 soup 객체가 없어 카테고리를 찾을 수 없습니다.\")"
      ],
      "id": "-UzIZnDmCWUA_student_func_test"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaQWRoamCWUB"
      },
      "source": [
        "**2.2. 특정 카테고리 내 모든 책 URL 수집 (페이지네이션 처리 포함)**\n",
        "\n",
        "카테고리 페이지에는 여러 권의 책이 나열되며, 책이 많을 경우 여러 페이지에 걸쳐 표시됩니다 ('Next' 버튼 존재).\n",
        "하나의 카테고리 URL을 입력받아, 해당 카테고리의 모든 페이지를 방문하며 각 책의 상세 페이지 URL을 수집하는 함수를 작성합니다.\n",
        "여기서는 학습을 위해 각 카테고리당 최대 페이지 수를 제한할 수 있습니다."
      ],
      "id": "eaQWRoamCWUB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKJEECu9CWUB_student_func_def"
      },
      "outputs": [],
      "source": [
        "def get_book_urls_from_category(category_url, max_pages_per_category=5):\n",
        "    \"\"\"특정 카테고리 페이지(및 하위 페이지)에서 모든 책의 상세 URL을 수집하는 함수\"\"\"\n",
        "    book_urls = set() # 중복 방지를 위해 set 사용\n",
        "    current_page_url = category_url\n",
        "    pages_crawled = 0\n",
        "\n",
        "    while current_page_url and pages_crawled < max_pages_per_category:\n",
        "        category_soup = ____(current_page_url) # get_soup\n",
        "        pages_crawled += 1\n",
        "\n",
        "        if not category_soup:\n",
        "            print(f\"카테고리 페이지 로드 실패: {current_page_url}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        for book_pod in category_soup.select(____): # CSS 선택자 for product_pod\n",
        "\n",
        "\n",
        "        # 다음 페이지(Next) 버튼 찾기\n",
        "        next_button = category_soup.select_one(____) # CSS 선택자 for next button\n",
        "        if next_button and next_button.has_attr('href'):\n",
        "\n",
        "        else:\n",
        "\n",
        "\n",
        "    return list(book_urls)"
      ],
      "id": "bKJEECu9CWUB_student_func_def"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKJEECu9CWUB_student_test_setup"
      },
      "outputs": [],
      "source": [
        "# 테스트: 첫 번째 카테고리에 대해서만 책 URL 수집 (최대 2페이지만)\n",
        "all_book_detail_urls = set()"
      ],
      "id": "bKJEECu9CWUB_student_test_setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKJEECu9CWUB_student_test_logic"
      },
      "outputs": [],
      "source": [
        "if all_categories:\n",
        "\n",
        "else:\n",
        "    print(\"카테고리 정보가 없어 책 URL을 수집할 수 없습니다.\")"
      ],
      "id": "bKJEECu9CWUB_student_test_logic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl2gLIr5CWUC"
      },
      "source": [
        "## 📖 3단계: 개별 책 상세 정보 스크래핑\n",
        "\n",
        "수집된 각 책의 상세 페이지 URL로 접속하여 책의 제목, 가격, 재고 상태, 상품 설명, UPC, 별점 등 다양한 정보를 추출합니다."
      ],
      "id": "zl2gLIr5CWUC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhETfZr8CWUC"
      },
      "source": [
        "**3.1. 책 상세 정보 추출 함수 정의**\n",
        "\n",
        "이 함수는 하나의 책 상세 페이지 URL을 입력받아 해당 페이지의 모든 필요한 정보를 추출하여 딕셔너리 형태로 반환합니다.\n",
        "정보가 누락된 경우를 대비해 오류 처리 및 기본값 설정을 포함합니다."
      ],
      "id": "HhETfZr8CWUC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc20p1I5CWUC_student_func_def"
      },
      "outputs": [],
      "source": [
        "def scrape_book_details(book_url):\n",
        "    \"\"\"개별 책 상세 페이지에서 정보를 추출하는 함수\"\"\"\n",
        "    book_data = {'url': book_url} # 기본 정보로 URL 추가\n",
        "    soup = ____(book_url) # get_soup\n",
        "\n",
        "    if not soup:\n",
        "        print(f\"책 상세 정보 로드 실패: {book_url}\")\n",
        "        book_data['title'] = \"정보 없음 (로드 실패)\"\n",
        "        return book_data\n",
        "\n",
        "    # 제목 (h1 태그)\n",
        "    title_tag = soup.select_one(____) # CSS 선택자 for title\n",
        "    book_data['title'] = title_tag.____(strip=True) if title_tag else None # get_text\n",
        "\n",
        "    # 가격 (p.price_color)\n",
        "    price_tag = soup.select_one(____) # CSS 선택자 for price\n",
        "    if price_tag:\n",
        "\n",
        "    else:\n",
        "\n",
        "\n",
        "    # 재고 상태 (p.instock.availability)\n",
        "    stock_tag = soup.select_one(____) # CSS 선택자 for stock\n",
        "    if stock_tag:\n",
        "\n",
        "    else:\n",
        "\n",
        "\n",
        "    # 별점 (p.star-rating 에 붙은 클래스 이름으로 확인. 예: \"star-rating Four\")\n",
        "    star_rating_tag = soup.select_one(____) # CSS 선택자 for star rating\n",
        "    if star_rating_tag and star_rating_tag.has_attr(____): # 'class'\n",
        "\n",
        "    else:\n",
        "        book_data['star_rating'] = None\n",
        "\n",
        "    # 상품 설명 (div#product_description + p)\n",
        "    desc_header = soup.____('div', id=____) # find, 'product_description'\n",
        "    if desc_header and desc_header.____('p'): # find_next_sibling\n",
        "        book_data['description'] = desc_header.find_next_sibling('p').____(strip=True) # get_text\n",
        "    else:\n",
        "        book_data['description'] = None\n",
        "\n",
        "    # 상품 정보 테이블 (UPC, Product Type, Price (excl. tax), etc.)\n",
        "    product_info_table = soup.____('table', class_=____) # find, 'table-striped'\n",
        "    if product_info_table:\n",
        "        for row in product_info_table.find_all('tr'):\n",
        "            header = row.find('th').____(strip=True) if row.find('th') else None # get_text\n",
        "            value = row.find('td').____(strip=True) if row.find('td') else None # get_text\n",
        "            if header and value:\n",
        "                clean_header = ____(____, '', header.lower().replace(' ', '_')) # re.sub, regex pattern for cleaning header\n",
        "                book_data[clean_header] = value\n",
        "                if header == ____: book_data['upc'] = value # 'UPC'\n",
        "                if header == 'Availability': book_data['stock_from_table'] = value\n",
        "                if header == 'Number of reviews': book_data['num_reviews'] = int(value) if value.isdigit() else 0\n",
        "    return book_data"
      ],
      "id": "pc20p1I5CWUC_student_func_def"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc20p1I5CWUC_student_test_setup"
      },
      "outputs": [],
      "source": [
        "# 함수 테스트 (이전에 수집한 책 URL 중 하나 사용)\n",
        "if all_book_detail_urls:\n",
        "    sample_book_url = list(all_book_detail_urls)[____] # index (e.g. 0)"
      ],
      "id": "pc20p1I5CWUC_student_test_setup"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc20p1I5CWUC_student_test_call"
      },
      "outputs": [],
      "source": [
        "    print(f\"\\n샘플 책 상세 정보 스크래핑: {sample_book_url}\")\n",
        "    detailed_book_info = ____(sample_book_url) # scrape_book_details"
      ],
      "id": "pc20p1I5CWUC_student_test_call"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc20p1I5CWUC_student_test_print"
      },
      "outputs": [],
      "source": [
        "    if detailed_book_info:\n",
        "        for key, value in detailed_book_info.items():\n",
        "            if key == 'description' and value and len(value) > 70:\n",
        "                print(f\"{key}: {value[:70]}...\")\n",
        "            else:\n",
        "                print(f\"{key}: {value}\")\n",
        "else:\n",
        "    print(\"\\n스크래핑할 책 URL이 없습니다.\")"
      ],
      "id": "pc20p1I5CWUC_student_test_print"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uInB9vLuCWUD"
      },
      "source": [
        "**3.2. 수집된 모든 책 URL에 대해 상세 정보 스크래핑 실행**\n",
        "\n",
        "이제 `get_book_urls_from_category` 함수로 모든 카테고리에서 책 URL들을 수집하고 (또는 선택한 몇 개 카테고리에서만), 각 URL에 대해 `scrape_book_details` 함수를 호출하여 전체 책 데이터를 만듭니다.\n",
        "실제 운영 시에는 매우 많은 요청이 발생할 수 있으므로, 여기서는 **일부 카테고리**와 각 카테고리당 **제한된 페이지 수**, 그리고 **전체 책 수 제한**을 두어 실습합니다."
      ],
      "id": "uInB9vLuCWUD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyou97UVCWUD_student_setup_vars"
      },
      "outputs": [],
      "source": [
        "final_book_data_list = []\n",
        "all_target_book_urls = set() # 최종적으로 스크랩할 책 URL들 (중복 제거)\n",
        "\n",
        "# 수집 대상 카테고리 수 제한 (예: 처음 2개 카테고리만)\n",
        "categories_to_scrape = all_categories[____] if all_categories else [] # slice, e.g. :2\n",
        "MAX_BOOKS_TO_SCRAPE_TOTAL = ____ # 전체 스크래핑할 책의 최대 개수 (테스트용, e.g. 10)"
      ],
      "id": "oyou97UVCWUD_student_setup_vars"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyou97UVCWUD_student_collect_urls_loop"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n총 {len(categories_to_scrape)}개 카테고리에서 책 URL 수집 시작...\")\n",
        "for category in categories_to_scrape:\n",
        "    print(f\"'{category['name']}' 카테고리 처리 중...\")\n",
        "    # 각 카테고리당 최대 1 페이지만 (시간 절약 위해)\n",
        "    book_urls = ____(category['url'], max_pages_per_category=____) # get_book_urls_from_category, page limit (e.g. 1)\n",
        "    for url in book_urls:\n",
        "        all_target_book_urls.add(url)\n",
        "        if len(all_target_book_urls) >= ____: # MAX_BOOKS_TO_SCRAPE_TOTAL\n",
        "            break # 전체 책 수 제한 도달 시 URL 수집 중단\n",
        "    if len(all_target_book_urls) >= ____: # MAX_BOOKS_TO_SCRAPE_TOTAL\n",
        "        print(\"전체 스크래핑할 책의 최대 개수에 도달했습니다.\")\n",
        "        break"
      ],
      "id": "oyou97UVCWUD_student_collect_urls_loop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyou97UVCWUD_student_scrape_details_loop"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n총 {len(all_target_book_urls)}개의 책 상세 정보를 스크래핑합니다.\")\n",
        "for i, book_url in enumerate(list(all_target_book_urls)):\n",
        "    print(f\"--- {i+1}/{len(all_target_book_urls)}번째 책 스크래핑 시작 --- \")\n",
        "    book_info = ____(book_url) # scrape_book_details\n",
        "    if book_info:\n",
        "        final_book_data_list.append(book_info)"
      ],
      "id": "oyou97UVCWUD_student_scrape_details_loop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyou97UVCWUD_student_print_results"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n총 {len(final_book_data_list)}개의 책 상세 정보 수집 완료.\")\n",
        "if final_book_data_list:\n",
        "    print(\"수집된 첫 번째 책 정보:\")\n",
        "    print(final_book_data_list[0])"
      ],
      "id": "oyou97UVCWUD_student_print_results"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5fQa61TCWUE"
      },
      "source": [
        "## 💾 4단계: 데이터 통합 및 저장\n",
        "\n",
        "수집된 모든 책 정보를 Pandas DataFrame으로 변환하여 분석 및 저장이 용이하도록 합니다."
      ],
      "id": "P5fQa61TCWUE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dus2QTNCWUE"
      },
      "source": [
        "**4.1. Pandas DataFrame으로 변환**\n",
        "\n",
        "리스트 형태로 저장된 책 데이터(`final_book_data_list`)를 DataFrame으로 만듭니다."
      ],
      "id": "9dus2QTNCWUE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m81dfW0DCWUG_student_create_df"
      },
      "outputs": [],
      "source": [
        "if final_book_data_list:\n",
        "    df_books = pd.____(final_book_data_list) # DataFrame"
      ],
      "id": "m81dfW0DCWUG_student_create_df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m81dfW0DCWUG_student_df_options"
      },
      "outputs": [],
      "source": [
        "    print(\"\\n--- Pandas DataFrame 변환 결과 (상위 5개) ---\")\n",
        "    pd.set_option(____, None) # 'display.max_columns'\n",
        "    pd.set_option(____, ____)      # 'display.width', 1000"
      ],
      "id": "m81dfW0DCWUG_student_df_options"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m81dfW0DCWUG_student_df_head"
      },
      "outputs": [],
      "source": [
        "    print(df_books.____()) # head"
      ],
      "id": "m81dfW0DCWUG_student_df_head"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m81dfW0DCWUG_student_df_info"
      },
      "outputs": [],
      "source": [
        "    print(\"\\n--- DataFrame 정보 ---\")\n",
        "    df_books.____() # info"
      ],
      "id": "m81dfW0DCWUG_student_df_info"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m81dfW0DCWUG_student_df_describe"
      },
      "outputs": [],
      "source": [
        "    print(\"\\n--- 기술 통계 (숫자형 데이터) ---\")\n",
        "    print(df_books.____(include=____)) # describe, 'number'"
      ],
      "id": "m81dfW0DCWUG_student_df_describe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m81dfW0DCWUG_student_else_empty_df"
      },
      "outputs": [],
      "source": [
        "else:\n",
        "    print(\"\\n수집된 데이터가 없어 DataFrame으로 변환할 수 없습니다.\")\n",
        "    df_books = pd.DataFrame() # 빈 DataFrame 생성"
      ],
      "id": "m81dfW0DCWUG_student_else_empty_df"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKZpoqX9CWUG"
      },
      "source": [
        "**4.2. CSV 파일로 저장**\n",
        "\n",
        "생성된 DataFrame을 CSV 파일로 저장하여 나중에 사용하거나 다른 도구로 분석할 수 있도록 합니다."
      ],
      "id": "iKZpoqX9CWUG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJZp4MzCWUH_student_if_not_empty"
      },
      "outputs": [],
      "source": [
        "if not df_books.empty:"
      ],
      "id": "vPJZp4MzCWUH_student_if_not_empty"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJZp4MzCWUH_student_filename"
      },
      "outputs": [],
      "source": [
        "    csv_filename = ____ # 'books_scraped_advanced.csv'"
      ],
      "id": "vPJZp4MzCWUH_student_filename"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJZp4MzCWUH_student_try_save"
      },
      "outputs": [],
      "source": [
        "    try:\n",
        "        df_books.____(____, index=____, encoding=____) # to_csv, csv_filename, False, 'utf-8-sig'\n",
        "        print(f\"\\n'{csv_filename}' 파일로 저장 완료!\")\n",
        "        print(\"Colab 환경의 좌측 '파일' 탭에서 해당 파일을 확인하고 다운로드할 수 있습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CSV 파일 저장 중 오류 발생: {e}\")"
      ],
      "id": "vPJZp4MzCWUH_student_try_save"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPJZp4MzCWUH_student_else_no_data"
      },
      "outputs": [],
      "source": [
        "else:\n",
        "    print(\"\\n저장할 데이터가 없습니다.\")"
      ],
      "id": "vPJZp4MzCWUH_student_else_no_data"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15KgstF9CWUH"
      },
      "source": [],
      "id": "15KgstF9CWUH"
    }
  ]
}