{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woghd8503/kernel-academy-web-crawling/blob/main/bsoup_fastcampus_middle_%ED%95%99%EC%83%9D%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rIgGzcp_vMX"
      },
      "source": [
        "# 웹 데이터 수집 중급: BeautifulSoup과 Requests로 Quotes to Scrape 크롤링 quote️ (중급)\n",
        "\n",
        " 지난 시간에는 BeautifulSoup의 기본 사용법을 익혔습니다. 이번 중급 과정에서는 실제 웹사이트인 'Quotes to Scrape' ([http://quotes.toscrape.com/](http://quotes.toscrape.com/)) 사이트의 데이터를 수집해 보겠습니다.\n",
        "\n",
        "**학습 목표:**\n",
        "1. `requests` 라이브러리를 사용하여 웹 페이지의 HTML 내용 가져오기\n",
        "2. 웹사이트에서 명언, 저자, 관련 태그 정보 추출하기\n",
        "3. 여러 페이지에 걸친 데이터 수집 방법 (페이지네이션 처리 기초) 이해하기\n",
        "4. CSS 선택자를 활용한 요소 검색 맛보기\n",
        "5. 수집한 데이터를 Pandas DataFrame으로 간단히 변환하기\n",
        "\n",
        "**주의:** 웹 스크래핑 시에는 항상 해당 웹사이트의 이용 약관 및 `robots.txt` 파일을 확인하여 서버에 부담을 주지 않는 범위 내에서 합법적으로 데이터를 수집해야 합니다. 'Quotes to Scrape'는 학습 목적으로 스크래핑이 허용된 사이트입니다.\n",
        "\n",
        "---"
      ],
      "id": "9rIgGzcp_vMX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2XxM4GF_vMY"
      },
      "source": [
        "## 1. 라이브러리 준비 및 첫 페이지 HTML 가져오기 🌐\n",
        "\n",
        "웹 페이지 내용을 가져오기 위해 `requests` 라이브러리가 필요합니다. BeautifulSoup은 HTML을 *파싱*하는 도구이지, 웹에서 직접 HTML을 *가져오는* 기능은 없습니다.\n",
        "\n",
        "1.  `requests`와 `beautifulsoup4`, 그리고 데이터 처리를 위한 `pandas`를 설치합니다. (Colab에는 대부분 미리 설치되어 있습니다.)\n",
        "2.  필요한 라이브러리들을 import 합니다.\n",
        "3.  `requests.get(URL)` 함수를 사용하여 'Quotes to Scrape' 사이트의 HTML을 가져옵니다.\n",
        "4.  가져온 HTML을 `BeautifulSoup` 객체로 만듭니다.\n",
        "5.  `urllib.parse.urljoin`은 상대 경로 URL을 절대 경로로 만들기 위해 사용됩니다."
      ],
      "id": "h2XxM4GF_vMY"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin # URL을 조합하기 위해 import 합니다."
      ],
      "metadata": {
        "id": "9awRem_lBtyV"
      },
      "id": "9awRem_lBtyV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스크래핑할 기본 URL\n",
        "base_url = 'http://quotes.toscrape.com/'"
      ],
      "metadata": {
        "id": "adqDYo7QBy3j"
      },
      "id": "adqDYo7QBy3j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = '안녕하세요?'"
      ],
      "metadata": {
        "id": "VP-4mns4ChUN"
      },
      "id": "VP-4mns4ChUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 페이지 HTML 가져오기\n",
        "response = requests.get(base_url)\n",
        "# request -> 보내는것  , response -> 받는것 (리턴값)\n",
        "# get 방식 (url 로 직접 접근 방식 (리턴이 페이지로 넘어옴)) ,\n",
        "# post 방식 (회원 가입, 폼, 입력할때 정보만 전달하는 방식 (리턴이 필요 없는))\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFapHC_B7cG",
        "outputId": "e47a9970-4dc3-4e77-b6c4-819538b870fb"
      },
      "id": "KgFapHC_B7cG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB1Jvv2I_vMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e105d3-643b-4657-a5e0-b50d864fbc52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "페이지 제목: Quotes to Scrape\n",
            "첫 페이지 로드 성공!\n"
          ]
        }
      ],
      "source": [
        "# 요청이 성공했는지 확인 (상태 코드 200이면 성공)\n",
        "if response.status_code == 200:\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    print(f\"페이지 제목: {soup.title.string}\")\n",
        "    print(f\"첫 페이지 로드 성공!\")\n",
        "else:\n",
        "    print(f\"페이지 로드 실패: 상태 코드 {response.status_code}\")\n",
        "    # 실패 시 이후 코드 실행을 막기 위해 soup을 None으로 설정하거나 예외 발생 처리 가능\n",
        "    soup = None"
      ],
      "id": "PB1Jvv2I_vMY"
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "MtJaPGUNFMXb"
      },
      "id": "MtJaPGUNFMXb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxVP77fD_vMZ"
      },
      "source": [
        "---\n",
        "## 2. 첫 페이지에서 명언, 저자, 태그 추출하기 ✍️\n",
        "\n",
        "'Quotes to Scrape' 사이트를 보면 각 명언은 `div` 태그 중 `class`가 `quote`인 요소 안에 들어있습니다. 이 구조를 파악하여 정보를 추출합니다.\n",
        "\n",
        "* **명언(Quote)**: `div.quote` 내부의 `span.text` 요소\n",
        "* **저자(Author)**: `div.quote` 내부의 `small.author` 요소\n",
        "* **태그(Tags)**: `div.quote` 내부의 `div.tags` 안에 있는 `a.tag` 요소들\n",
        "\n",
        "먼저 `find_all()`을 사용해 모든 명언이 담긴 `div.quote` 요소들을 찾고, 각 요소 내부에서 명언, 저자, 태그를 추출합니다."
      ],
      "id": "xxVP77fD_vMZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u029YqDN_vMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee04e0c-b5c0-4d1b-fcea-545f4e96fb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 페이지에서 10개의 명언을 찾았습니다.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 모든 'div' 태그 중 class가 'quote'인 요소들을 찾습니다.\n",
        "quote_divs = soup.find_all('div', class_='quote')\n",
        "\n",
        "print(f\"첫 페이지에서 {len(quote_divs)}개의 명언을 찾았습니다.\\n\")\n",
        "quotes_data = [] # 추출한 데이터를 저장할 리스트"
      ],
      "id": "u029YqDN_vMZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# 명언 추출 for문을 써서\n",
        "for quote_div in quote_divs:\n",
        "    text = quote_div.find('span', class_='text').get_text(strip=True) # 앞 뒤 공백이 있다면, 제거해서 가져온다 - strip = True\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "XtCiRYK_cNN5"
      },
      "id": "XtCiRYK_cNN5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저자 추출\n",
        "for quote_div in quote_divs:\n",
        "    author = quote_div.find('small', class_='author').get_text()\n",
        "    print(author)"
      ],
      "metadata": {
        "id": "SB2cVMqQcWql"
      },
      "id": "SB2cVMqQcWql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 태그 추출 (여러 개일 수 있음)\n",
        "for quote_div in quote_divs:\n",
        "    tags_elements = quote_div.find_all('a' , class_= 'tag')\n",
        "    tags = [tag.get_text() for tag in tags_elements]\n",
        "    print(tags)"
      ],
      "metadata": {
        "id": "EcCSvDqqcXSz"
      },
      "id": "EcCSvDqqcXSz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i , quote_info in enumerate(quote_divs):\n",
        "    text = quote_info.find('span', class_='text').get_text(strip=True) # 앞 뒤 공백이 있다면, 제거해서 가져온다 - strip = True\n",
        "    author = quote_info.find('small', class_='author').get_text()\n",
        "    tags_elements = quote_info.find_all('a' , class_= 'tag')\n",
        "    tags = [tag.get_text() for tag in tags_elements]\n",
        "    quotes_data.append({\n",
        "        'text': text,\n",
        "        'author': author,\n",
        "        'tags': tags\n",
        "    })\n",
        "quotes_data"
      ],
      "metadata": {
        "id": "GbvudOcbKhaN"
      },
      "id": "GbvudOcbKhaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q:  enumerate가 뭔가요?\n",
        "\n",
        "testlist = [100,1,2,3,4,5,6,7]\n",
        "for idx,value in enumerate(testlist):\n",
        "    # enumerate 는 idx, value 같이 리턴\n",
        "    print(idx,value)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gLRSTZ-lPxsi"
      },
      "id": "gLRSTZ-lPxsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복문으로 통합해 보기!"
      ],
      "metadata": {
        "id": "K_-3GvJ_cmd8"
      },
      "id": "K_-3GvJ_cmd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kliK7rI_vMZ"
      },
      "source": [
        "---\n",
        "## 3. CSS 선택자로 요소 찾기 🎯\n",
        "\n",
        "BeautifulSoup은 `find()`나 `find_all()` 외에도 CSS 선택자(Selector)를 사용하여 요소를 찾는 `select()` 와 `select_one()` 메소드를 제공합니다. CSS 선택자는 HTML 구조를 경로처럼 지정하여 요소를 선택하는 강력한 방법입니다.\n",
        "\n",
        "* `soup.select('선택자')`: 선택자와 일치하는 모든 요소를 리스트로 반환 ( `find_all`과 유사)\n",
        "* `soup.select_one('선택자')`: 선택자와 일치하는 첫 번째 요소를 반환 ( `find`와 유사)\n",
        "\n",
        "예시:\n",
        "* `div.quote`: class가 `quote`인 `div` 태그\n",
        "* `span.text`: class가 `text`인 `span` 태그\n",
        "* `div.tags a.tag`: class가 `tags`인 `div` 태그 내부의 class가 `tag`인 모든 `a` 태그\n",
        "\n",
        "앞서 `find`와 `find_all`로 했던 작업을 `select`를 사용해서 다시 해봅시다."
      ],
      "id": "0kliK7rI_vMZ"
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_data_with_selector = []"
      ],
      "metadata": {
        "id": "F9GkzGQKcjur"
      },
      "id": "F9GkzGQKcjur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CSS 선택자를 사용하여 모든 'div.quote' 요소들을 찾습니다.\n",
        "quote_divs_selected = soup.select('div.quote')"
      ],
      "metadata": {
        "id": "yFS9qgXJcjqZ"
      },
      "id": "yFS9qgXJcjqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CSS 선택자로 {len(quote_divs_selected)}개의 명언을 찾았습니다.\\n\")"
      ],
      "metadata": {
        "id": "ggEWqhjYcjjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2353df-11e2-4bab-83ee-79674a2bdfc7"
      },
      "id": "ggEWqhjYcjjz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSS 선택자로 10개의 명언을 찾았습니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명언 추출 (select_one 사용)\n",
        "text = quote_div.select_one('span.text').get_text()\n",
        "text"
      ],
      "metadata": {
        "id": "-0ZCsZqjcjav",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78568164-656a-4ac9-f641-c918f7f0c267"
      },
      "id": "-0ZCsZqjcjav",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'“A day without sunshine is like, you know, night.”'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저자 추출\n",
        "author = quote_div.select_one('small.author').get_text()\n",
        "author"
      ],
      "metadata": {
        "id": "ToEH2OyTc0qh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2cff9434-5e8a-40fb-c54b-fb9bd5a88b81"
      },
      "id": "ToEH2OyTc0qh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Steve Martin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 태그 추출 (select 사용)\n",
        "tags_elements = quote_div.select('div.tags a.tag')\n",
        "#tags_elements = tags_elements.select('a.tag')\n",
        "tags = [tag.get_text() for tag in tags_elements]\n",
        "\n",
        "quotes_data_with_selector.append({\n",
        "    'text' : text,\n",
        "    'author' : author,\n",
        "    'tags' : tags\n",
        "})\n",
        "print(quotes_data_with_selector)"
      ],
      "metadata": {
        "id": "DDatafYWc1qQ"
      },
      "id": "DDatafYWc1qQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, quote_info in enumerate(quotes_data_with_selector[:2]):\n",
        "    print(f\"--- 선택자 명언 {i+1} ---\")\n",
        "    print(f\"내용: {quote_info['text']}\")\n",
        "    print(f\"저자: {quote_info['author']}\")\n",
        "    print(f\"태그: {', '.join(quote_info['tags'])}\\n\")"
      ],
      "metadata": {
        "id": "mNNYdTgGc3y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7c389b-bb90-4535-db6d-8a049c8a085b"
      },
      "id": "mNNYdTgGc3y7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 선택자 명언 1 ---\n",
            "내용: “A day without sunshine is like, you know, night.”\n",
            "저자: Steve Martin\n",
            "태그: humor, obvious, simile\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 통합해보기!"
      ],
      "metadata": {
        "id": "W6YTduL6c6Vi"
      },
      "id": "W6YTduL6c6Vi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaEIiY_b_vMZ"
      },
      "source": [
        "---\n",
        "## 4. 여러 페이지 데이터 수집 (페이지네이션) 📄➡️📄\n",
        "\n",
        "많은 웹사이트는 한 페이지에 모든 정보를 보여주지 않고 여러 페이지로 나눕니다. 'Quotes to Scrape'도 마찬가지로 'Next →' 버튼이 있어 다음 페이지로 이동할 수 있습니다.\n",
        "\n",
        "페이지네이션을 처리하는 일반적인 방법\n",
        "1.  현재 페이지에서 '다음(Next)' 버튼(또는 링크)을 찾습니다.\n",
        "2.  '다음' 버튼이 있다면 해당 링크(URL)를 가져옵니다. 링크가 상대 경로일 경우, 기본 URL과 조합하여 절대 경로로 만듭니다. (`urllib.parse.urljoin` 사용)\n",
        "3.  새 URL로 접속하여 위에서 했던 정보 추출 과정을 반복합니다.\n",
        "4.  '다음' 버튼이 더 이상 없을 때까지 반복합니다.\n",
        "\n",
        "여기서는  **최대 3 페이지**까지만 데이터를 수집해보겠습니다."
      ],
      "id": "qaEIiY_b_vMZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Tg6ptlA_vMa"
      },
      "outputs": [],
      "source": [
        "all_quotes_data = [] # 모든 페이지의 명언을 저장할 리스트\n",
        "current_url = base_url\n",
        "max_pages = 100"
      ],
      "id": "3Tg6ptlA_vMa"
    },
    {
      "cell_type": "code",
      "source": [
        "for page_num in range(max_pages):\n",
        "    print(f\"\\n{page_num + 1}번째 페이지 스크래핑 중: {current_url}\")\n",
        "    response = requests.get(current_url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"\\n{page_num + 1}번째 페이지 스크래핑 중 오류: {current_url}\")\n",
        "\n",
        "    soup = BeautifulSoup(response.text , 'html.parser')\n",
        "\n",
        "    quote_divs = soup.select('div.quote') # 명언 있는지 없는지 검토\n",
        "    if not quote_divs:\n",
        "        print('명언이 더이상 없습니다.')\n",
        "\n",
        "    for quote_div in quote_divs:\n",
        "        text = quote_div.select_one('span.text').get_text()\n",
        "        author =  quote_div.select_one('small.author').get_text()\n",
        "        tags = [tag.get_text() for tag in quote_div.select('div.tags a.tag')]\n",
        "        all_quotes_data.append({'text' : text , 'author' : author, 'tags' : tags})\n",
        "\n",
        "    next_li = soup.select_one('li.next')\n",
        "    if next_li:\n",
        "        next_page_relative_url = next_li.select_one('a')['href']\n",
        "        current_url = urljoin(base_url,next_page_relative_url)\n",
        "        #current_url = base_url + next_page_relative_url\n",
        "\n",
        "    else:\n",
        "        print('끝 페이지 입니다')\n",
        "        break\n",
        "print(f\"\\n총 {len(all_quotes_data)}개의 명언을 수집했습니다.\")\n"
      ],
      "metadata": {
        "id": "wFgyrL7Mc_K9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c04fc9-a133-40c9-b42d-d720bf8367b2"
      },
      "id": "wFgyrL7Mc_K9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1번째 페이지 스크래핑 중: http://quotes.toscrape.com/\n",
            "\n",
            "2번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/2/\n",
            "\n",
            "3번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/3/\n",
            "\n",
            "4번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/4/\n",
            "\n",
            "5번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/5/\n",
            "\n",
            "6번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/6/\n",
            "\n",
            "7번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/7/\n",
            "\n",
            "8번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/8/\n",
            "\n",
            "9번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/9/\n",
            "\n",
            "10번째 페이지 스크래핑 중: http://quotes.toscrape.com/page/10/\n",
            "끝 페이지 입니다\n",
            "\n",
            "총 100개의 명언을 수집했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 수집된 데이터 중 처음 5개와 마지막 2개 출력\n",
        "if len(all_quotes_data) > 0:\n",
        "    print(\"\\n--- 수집된 명언 (일부) ---\")\n",
        "    for i, quote_info in enumerate(all_quotes_data[:5]): # 처음 5개\n",
        "        print(f\"{i+1}. {quote_info['author']}: {quote_info['text'][:30]}... [{', '.join(quote_info['tags'])}]\")\n",
        "    if len(all_quotes_data) > 5:\n",
        "        print(\"...\")\n",
        "        for i, quote_info in enumerate(all_quotes_data[-2:]): # 마지막 2개\n",
        "            print(f\"{len(all_quotes_data) - 1 + i}. {quote_info['author']}: {quote_info['text'][:30]}... [{', '.join(quote_info['tags'])}]\")"
      ],
      "metadata": {
        "id": "TfEGQTGadFDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee60695-7136-4e57-d054-4312470aa2a2"
      },
      "id": "TfEGQTGadFDk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 수집된 명언 (일부) ---\n",
            "1. Albert Einstein: “The world as we have created ... [change, deep-thoughts, thinking, world]\n",
            "2. J.K. Rowling: “It is our choices, Harry, tha... [abilities, choices]\n",
            "3. Albert Einstein: “There are only two ways to li... [inspirational, life, live, miracle, miracles]\n",
            "4. Jane Austen: “The person, be it gentleman o... [aliteracy, books, classic, humor]\n",
            "5. Marilyn Monroe: “Imperfection is beauty, madne... [be-yourself, inspirational]\n",
            "...\n",
            "29. Albert Einstein: “Logic will get you from A to ... [imagination]\n",
            "30. Bob Marley: “One good thing about music, w... [music]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-402KpZ_vMa"
      },
      "source": [
        "---\n",
        "## 5. 수집한 데이터를 Pandas DataFrame으로 변환 및 저장하기 📊💾\n",
        "\n",
        "지금까지 리스트 형태로 수집한 명언 데이터를 다루기 쉽게 Pandas DataFrame으로 변환할 수 있습니다. DataFrame으로 변환하면 정렬, 필터링, 분석 등이 용이해지고, CSV 파일 등으로 쉽게 저장할 수 있습니다.\n",
        "\n",
        "** Pandas DataFrame 주요 특징 **\n",
        "* 2차원 테이블 형태의 자료구조\n",
        "* 각 열(column)은 서로 다른 타입을 가질 수 있음 (숫자, 문자열, 불리언 등)\n",
        "* 데이터 분석 및 전처리에 매우 유용\n",
        "\n",
        "여기서는 수집한 `all_quotes_data` 리스트를 DataFrame으로 만들고, 처음 몇 줄을 확인한 뒤 CSV 파일로 저장하는 예시를 보여줍니다."
      ],
      "id": "h-402KpZ_vMa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSer8YyY_vMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ade9819-b600-4729-e2c4-24c237f7ebee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Pandas DataFrame 변환 결과 (상위 5개) ---\n",
            "                                                 text              author  \\\n",
            "0   “The world as we have created it is a process ...     Albert Einstein   \n",
            "1   “It is our choices, Harry, that show what we t...        J.K. Rowling   \n",
            "2   “There are only two ways to live your life. On...     Albert Einstein   \n",
            "3   “The person, be it gentleman or lady, who has ...         Jane Austen   \n",
            "4   “Imperfection is beauty, madness is genius and...      Marilyn Monroe   \n",
            "..                                                ...                 ...   \n",
            "95  “You never really understand a person until yo...          Harper Lee   \n",
            "96  “You have to write the book that wants to be w...   Madeleine L'Engle   \n",
            "97  “Never tell the truth to people who are not wo...          Mark Twain   \n",
            "98        “A person's a person, no matter how small.”           Dr. Seuss   \n",
            "99  “... a mind needs books as a sword needs a whe...  George R.R. Martin   \n",
            "\n",
            "                                                 tags  \n",
            "0            [change, deep-thoughts, thinking, world]  \n",
            "1                                [abilities, choices]  \n",
            "2      [inspirational, life, live, miracle, miracles]  \n",
            "3                  [aliteracy, books, classic, humor]  \n",
            "4                        [be-yourself, inspirational]  \n",
            "..                                                ...  \n",
            "95                              [better-life-empathy]  \n",
            "96  [books, children, difficult, grown-ups, write,...  \n",
            "97                                            [truth]  \n",
            "98                                    [inspirational]  \n",
            "99                                      [books, mind]  \n",
            "\n",
            "[100 rows x 3 columns]\n",
            "\n",
            "DataFrame 형태: (100, 3) (행, 열)\n",
            "\n",
            "'quotes_scraped.csv' 파일로 저장 완료!\n",
            "웹 스크래핑 시에는 항상 웹사이트의 robots.txt와 이용 약관을 준수하고, 서버에 과도한 부하를 주지 않도록 주의하세요.\n"
          ]
        }
      ],
      "source": [
        "if all_quotes_data: # 데이터가 있을 경우에만 실행\n",
        "    # 리스트를 Pandas DataFrame으로 변환\n",
        "    df_quotes = pd.DataFrame(all_quotes_data)\n",
        "\n",
        "    print(\"\\n--- Pandas DataFrame 변환 결과 (상위 5개) ---\")\n",
        "    print(df_quotes)\n",
        "\n",
        "    print(f\"\\nDataFrame 형태: {df_quotes.shape} (행, 열)\")\n",
        "\n",
        "    # CSV 파일로 저장 (index=False는 DataFrame의 인덱스를 파일에 쓰지 않도록 함)\n",
        "    # Colab 환경에서는 이 파일이 세션 저장 공간에 임시로 저장됩니다.\n",
        "    # 좌측 파일 탐색기에서 확인할 수 있으며, 로컬로 다운로드 가능합니다.\n",
        "    csv_filename = 'quotes_scraped.csv'\n",
        "    df_quotes.to_csv(csv_filename, index=False, encoding='utf-8-sig') # utf-8-sig로 한글 깨짐 방지\n",
        "    print(f\"\\n'{csv_filename}' 파일로 저장 완료!\")\n",
        "\n",
        "    # # 파일 내용 간단히 확인 (Colab에서 바로 확인하기 위함)\n",
        "    # print(\"\\n--- CSV 파일 내용 (처음 5줄) ---\")\n",
        "    # with open(csv_filename, 'r', encoding='utf-8-sig') as f:\n",
        "    #     for i, line in enumerate(f):\n",
        "    #         if i < 5:\n",
        "    #             print(line.strip())\n",
        "    #         else:\n",
        "    #             break\n",
        "else:\n",
        "    print(\"\\n수집된 데이터가 없어 DataFrame으로 변환할 수 없습니다.\")\n",
        "print(\"웹 스크래핑 시에는 항상 웹사이트의 robots.txt와 이용 약관을 준수하고, 서버에 과도한 부하를 주지 않도록 주의하세요.\")"
      ],
      "id": "VSer8YyY_vMa"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCg6rq5YdeVW"
      },
      "id": "KCg6rq5YdeVW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhOmz57uc_zg"
      },
      "id": "mhOmz57uc_zg",
      "execution_count": null,
      "outputs": []
    }
  ]
}